Загрузка и обработка размеченного датасета (рис. 1). В качестве размеченного датсета на русском языке с разметкой по негативности комментария (позитивные/негативные) был взять датасет сообщений из Twitter.
Датасет (англ. dataset) - это набор данных, обычно структурированных и организованных в таблицы или другие формы, которые используются для анализа и исследования в различных областях, таких как машинное обучение, статистика, биоинформатика, экономика и т.д.
Датасет может включать в себя информацию о различных переменных, таких как имена, возраст, пол, местоположение, здоровье и т.д. в зависимости от того, для какой области он предназначен. Данные в датасете могут быть представлены в различных форматах, таких как числа, текст, изображения или звуковые файлы.
Датасеты используются для различных целей, таких как обучение моделей машинного обучения, проверка гипотез, поиск закономерностей и тенденций в данных, анализ поведения пользователей и многое другое.
Датафрейм (англ. dataframe) - это структурированная форма представления данных в виде таблицы, которая используется в различных языках программирования, включая Python и R. Датафрейм представляет собой двумерный массив данных, где каждый столбец может иметь свой тип данных, такой как числа, строки или булевы значения.
В датафрейме данные организованы по рядам и столбцам, где каждый ряд представляет отдельное наблюдение, а каждый столбец - переменную или признак, характеризующую каждое наблюдение. Датафреймы обычно используются для анализа и обработки данных в различных областях, таких как статистика, экономика, биоинформатика, машинное обучение и т.д.
Преимущество использования датафреймов заключается в том, что они могут содержать данные различных типов, а также предоставлять удобный доступ к отдельным элементам и подмножествам данных. Они также предоставляют множество функций и методов для анализа данных, включая сортировку, фильтрацию, группировку, агрегирование и т.д.
Библиотека Natural Language Toolkit (NLTK) для языковых процессов в Python содержит модуль, который обеспечивает поддержку поиска стоп-слов.
Стоп-слова - это слова, которые обычно не имеют существенного значения в тексте и могут быть опущены без ущерба для его смысла. Это могут быть слова-союзы, предлоги, местоимения и т.д.
Для использования модуля поиска стоп-слов в NLTK, необходимо выполнить следующие шаги:
Установить библиотеку NLTK с помощью pip или другого менеджера пакетов для Python.
Загрузить список стоп-слов для конкретного языка, используя функцию stopwords.words() и указав язык в качестве параметра.
Пример стоп-слов в русском языке: «всю», «ну», «для», «он», «больше», «ей».
Далее необходимо представить очищенные тексты в TF-IDF векторном представлении.
TF-IDF (term frequency-inverse document frequency) – это способ представления текстовых документов в виде числовых векторов, чтобы использовать их в алгоритмах машинного обучения.
Каждый вектор соответствует отдельному документу и содержит значения, которые показывают, насколько часто каждое слово появляется в документе (TF) и насколько уникально это слово для данного документа в контексте всего корпуса текстов (IDF).
Чем чаще слово встречается в документе, тем больше его значение в векторе TF-IDF. Однако, если слово также часто встречается в других документах, то его значение в векторе IDF будет ниже, что будет снижать его общее значение в векторе TF-IDF. Это позволяет выделить ключевые слова и понять, насколько они уникальны для каждого документа.
Предварительно необходимо разделить выборку на тестовую и обучающую (рис. 5), чтобы можно было производить тесты на уже размеченных данных, где мы точно знаем нужную тональность.
Получилось, что размер обучающей выборки составил 181467 документов, а тестовой – 45367, так как было указано, что размер тестовой выборки составляет 20 процентов от общей.
Для получения векторных представлений использовался TfidfVectorizer (рис. 6). Это инструмент, предоставляемый библиотекой scikit-learn в Python, который используется для преобразования текстовых документов в числовые векторы, которые могут быть использованы для анализа текстовых данных. TfidfVectorizer использует метод TF-IDF (Term Frequency-Inverse Document Frequency), который вычисляет важность каждого слова в документе, учитывая частоту его появления в документе и общую частоту его появления во всех документах.
Алгоритм работы: каждый документ представляется в виде вектора, где каждый элемент вектора соответствует весу слова в документе. Вес слова зависит от его частоты в документе и частоты во всех документах в коллекции. Таким образом, слова, которые часто встречаются в одном документе, но редко в других документах, имеют большой вес в этом документе, что позволяет выделить наиболее значимые слова в тексте.
То есть это конкретные слова, их значения idf, умноженные на количество встречаемости в этом тексте.
В качестве начала работы для анализа тональности была выбрана модель логической регрессии (рис. 7).
Логическая регрессия - это математический алгоритм, который помогает предсказать значение зависимой переменной (например, вероятность покупки товара) на основе независимых переменных (например, возраст, пол, доход).
В анализе тональности текста логическая регрессия может использоваться для определения, положительный или отрицательный отзыв написан об объекте. Для этого, текст отзыва преобразуется в числовые значения, которые используются как независимые переменные в модели логической регрессии. Зависимой переменной будет являться тональность отзыва (положительная или отрицательная). После обучения модели на наборе размеченных данных, она сможет предсказывать тональность новых отзывов.



